{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6tPMREiliKOV"
      },
      "source": [
        "# Tokenizador BERT\n",
        "Vamos a ver cómo funciona el tokenizado WordPiece que utiliza el modelo BERT. Usamos la librería `transformers`. Se puede instalar en Anaconda con  \n",
        "```\n",
        "conda install -c huggingface transformers\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUczPxqNiKOW"
      },
      "outputs": [],
      "source": [
        "#instalamos la librería (Google Colab)\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTAB7XKRiKOX"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "urRDUxoHiKOY"
      },
      "source": [
        "Exploramos el vocabulario del modelo `bert-base-cased`(inglés)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqLbs9kmiKOY"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I8GXcLbiKOZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.choice(list(tokenizer.vocab.keys()), 10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0gjtmW2DiKOZ"
      },
      "source": [
        "El modelo BERT utiliza un tokenizado subword de tipo *WordPiece*:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPnGOz-diKOa"
      },
      "outputs": [],
      "source": [
        "output = tokenizer.tokenize(\"the BERT tokenizer was created with a WordPiece model\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLZduHsSiKOa"
      },
      "outputs": [],
      "source": [
        "len(output)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n4EwggMiiKOa"
      },
      "source": [
        "Para utilizar un documento tokenizado en BERT tenemos que convertirlo en tokens con el método `tokenizer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MToPQqFuiKOb"
      },
      "outputs": [],
      "source": [
        "output = tokenizer(\"the BERT tokenizer was created with a WordPiece model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOPpKqUuiKOb"
      },
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWMoia_5iKOb"
      },
      "outputs": [],
      "source": [
        "output.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV72c58piKOc"
      },
      "outputs": [],
      "source": [
        "len(output.input_ids)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C0YJahhkiKOc"
      },
      "source": [
        "El tokenizador asigna cada token a su `token_id` correspondiente en el vocabulario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r51Y9gniKOc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "tokens = map(lambda t: {'token_id': t,\n",
        "                        'token': tokenizer.convert_ids_to_tokens(t)},\n",
        "             output.input_ids)\n",
        "\n",
        "pd.DataFrame(tokens)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8aII_77tiKOd"
      },
      "source": [
        "El modelo BERT añade unos tokens especiales al inicio y al final de cada documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vroKWKdFiKOe"
      },
      "outputs": [],
      "source": [
        "#tokens especiales\n",
        "tokenizer.convert_ids_to_tokens([0, 100, 101, 102, 103])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T-ibzjsniKOe"
      },
      "source": [
        "Tokenizado de pares de frases (para aplicaciones con dos documentos de entrada, como NLI o QA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3fPwGgliKOe"
      },
      "outputs": [],
      "source": [
        "\n",
        "output = tokenizer([[\"this is the first sentence\",\n",
        "    \"this is the second one\"]])\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2LM5U79iKOf"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(output['input_ids'][0]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3qqMpbm7iKOf"
      },
      "source": [
        "### Tokenizado de un corpus de documentos\n",
        "Cuando se tokeniza más de un documento, es necesario ajustar la longitud de todos al mismo número de tokens (con *padding/truncating*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtHem_q5iKOg"
      },
      "outputs": [],
      "source": [
        "text = [\n",
        "    \"This movie was great!\",\n",
        "    \"I hated this move, waste of time!\",\n",
        "    \"Epic?\"\n",
        "]\n",
        "\n",
        "encoded = tokenizer(text, padding=True, add_special_tokens=True)\n",
        "\n",
        "print(\"**Input IDs**\")\n",
        "for a in encoded.input_ids:\n",
        "    print(a)\n",
        "\n",
        "print(\"**Attention Mask**\")\n",
        "for a in encoded.attention_mask:\n",
        "    print(a)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i7rVHEdIiKOg"
      },
      "source": [
        "## Análisis del vocabulario\n",
        "El vocabulario contiene palabras completas y partes de palabra (tokens que comienzan por '##')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RN8oSF2iiKOg"
      },
      "outputs": [],
      "source": [
        "one_chars = []\n",
        "one_chars_hashes = []\n",
        "\n",
        "# recorremos todos los tokens del vocabulario\n",
        "for token in tokenizer.vocab.keys():\n",
        "    \n",
        "    # guardamos los tokens de un solo caracter\n",
        "    if len(token) == 1:\n",
        "        one_chars.append(token)\n",
        "    \n",
        "    # guardamos los tokens de un solo caracter precedidos de ##    \n",
        "    elif len(token) == 3 and token[0:2] == '##':\n",
        "        one_chars_hashes.append(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioXvQmfYiKOh"
      },
      "outputs": [],
      "source": [
        "print(one_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLUBV6X6iKOh"
      },
      "outputs": [],
      "source": [
        "print(one_chars_hashes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRb-jkmwiKOh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
        "\n",
        "# Measure the length of every token in the vocab.\n",
        "token_lengths = [len(token) for token in tokenizer.vocab.keys()]\n",
        "\n",
        "# Plot the number of tokens of each length.\n",
        "sns.countplot(x=token_lengths)\n",
        "plt.title('Longitud de los tokens del vocabulario')\n",
        "plt.xlabel('Longitud del token')\n",
        "plt.ylabel('Nº de Tokens')\n",
        "\n",
        "print('Longitud de token máxima:', max(token_lengths))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-WUXLx-miKOh"
      },
      "source": [
        "Ahora analizamos sólo las subpalabras (tokens que empiezan por `'##'` y no son caracteres únicos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76is7k7FiKOi"
      },
      "outputs": [],
      "source": [
        "num_subwords = 0\n",
        "\n",
        "subword_lengths = []\n",
        "\n",
        "# For each token in the vocabulary...\n",
        "for token in tokenizer.vocab.keys():\n",
        "    \n",
        "    # If it's a subword...\n",
        "    if len(token) >= 2 and token[0:2] == '##':\n",
        "        \n",
        "        # Tally all subwords\n",
        "        num_subwords += 1\n",
        "\n",
        "        # Measure the sub word length (without the hashes)\n",
        "        length = len(token) - 2\n",
        "\n",
        "        # Record the lengths.        \n",
        "        subword_lengths.append(length)\n",
        "\n",
        "sns.countplot(x=subword_lengths)\n",
        "plt.title('Longitud de los subwords del vocabulario (sin \"##\")')\n",
        "plt.xlabel('Longitud subtoken')\n",
        "plt.ylabel('Nº de subwords')\n",
        "\n",
        "print(f\"Número de subwords: {len(subword_lengths)}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OkEyzNGoiKOi"
      },
      "source": [
        "## Tokenizado en otros idiomas\n",
        "El modelo BERT en inglés no conoce las palabras del vocabulario español\n",
        "### Ejercicio\n",
        "Convierte en tokens la frase '*El modelo BERT en inglés no conoce el vocabulario español*' con el modelo BERT previamente cargado y con el modelo multilingüe `nlptown/bert-base-multilingual-uncased-sentiment`. Compara las diferencias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LsIwkvEiKOi"
      },
      "outputs": [],
      "source": [
        "#Modelo BERT inglés\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eoSZsV4riKOi"
      },
      "source": [
        "En cambio el mismo texto tokenizado con un modelo BERT multilingüe sí conoce las palabras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbmLaNbBiKOj"
      },
      "outputs": [],
      "source": [
        "#Modelo BERT multilingüe"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "f818427314f6e07d80a3b82dd0595cafccc14dc50f870f003aae34cda4c72f85"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
