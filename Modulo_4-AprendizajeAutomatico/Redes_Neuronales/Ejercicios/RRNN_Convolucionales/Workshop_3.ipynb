{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL8RQZGoqqQJ"
      },
      "source": [
        "# Workshop 3: Advanced Techniques\n",
        "\n",
        "In this workshop we will learn techniques to increase the performance of CNN and how to use state of the art architectures. The structure of the workshop will be the following:\n",
        "\n",
        "\n",
        "\n",
        "1.   Dropout\n",
        "2.   Batch Normalization\n",
        "3.   Data Augmentation\n",
        "4.   Transfer learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y91AuX2jrJDo"
      },
      "source": [
        "## 1. Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxUYGXkFPG98"
      },
      "source": [
        "# Import dependence for downloading CIFAR10\n",
        "from tensorflow import keras\n",
        "from keras.datasets import cifar10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaGDZgZkS9c1"
      },
      "source": [
        "(X_train, y_train), (X_testval, y_testval) = cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA-qauKITM-O"
      },
      "source": [
        "# Import dependence for handling arrays\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ5vh3g8Xh_j"
      },
      "source": [
        "# Change the range of pixels from [0 255] to [0 1]\n",
        "X_train_fl = X_train.astype('float32')\n",
        "X_testval_fl = X_testval.astype('float32')\n",
        "X_train_fl /= 255\n",
        "X_testval_fl /= 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj74QnR4Vbt1"
      },
      "source": [
        "# Import dependence for one-hot encoding\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nTRJ7yeVeN3"
      },
      "source": [
        "# One-hot encoding of labels\n",
        "onehot_enc = OneHotEncoder()\n",
        "y_train_oh = onehot_enc.fit_transform(y_train.reshape(X_train.shape[0], 1)).toarray()\n",
        "y_testval_oh = onehot_enc.fit_transform(y_testval.reshape(X_testval.shape[0], 1)).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry7H5BvPVl6D"
      },
      "source": [
        "# Show one-hot encoded labels shape\n",
        "print(\"Training one-hot encoded labels shape:\", y_train_oh.shape)\n",
        "print(\"Testing one-hot encoded labels shape:\", y_testval_oh.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PoZUM_sVoi6"
      },
      "source": [
        "# Divide testval in test and validation partitions\n",
        "samples_test_nb = int(X_testval.shape[0]/2)\n",
        "X_val = X_testval_fl[:samples_test_nb]\n",
        "y_val = y_testval_oh[:samples_test_nb]\n",
        "X_test = X_testval_fl[samples_test_nb:]\n",
        "y_test = y_testval_oh[samples_test_nb:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofgM7brWVsN1"
      },
      "source": [
        "# Show shapes of test and validation partitions\n",
        "print(\"Validation matrix shape:\", X_val.shape)\n",
        "print(\"Testing matrix shape:\", X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo5FvhBYubiQ"
      },
      "source": [
        "# Import depence for CNN\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrbz0cUmtuU7"
      },
      "source": [
        "# Define the model\n",
        "input_layer = Input(shape=(X_train.shape[1],X_train.shape[2], X_train.shape[3]))\n",
        "conv_layer_1 = Conv2D(filters=8, kernel_size=(3, 3), activation='relu')(input_layer)\n",
        "maxpool_layer_1 = MaxPool2D(pool_size=(2, 2))(conv_layer_1)\n",
        "conv_layer_2 = Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(maxpool_layer_1)\n",
        "maxpool_layer_2 = MaxPool2D(pool_size=(2, 2))(conv_layer_2)\n",
        "conv_layer_3 = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(maxpool_layer_2)\n",
        "flatten_layer = Flatten()(conv_layer_3)\n",
        "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
        "dropout = Dropout(rate=0.4)(dense_layer) # New layer (Dropout with how many neurons I want to turn off)\n",
        "output_layer = Dense(10, activation='softmax')(dropout)\n",
        "model= Model(inputs=input_layer, outputs=output_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzt4Th7iV1tF"
      },
      "source": [
        "# Show a summary of the model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFM5py6aV4DM"
      },
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a6viyT4V_9_"
      },
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train_fl, y_train_oh, epochs=50, batch_size=128,\n",
        "                    validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hsc8-QX1xVWL"
      },
      "source": [
        "# Import dependence for visualization of images\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAR15yICV6hU"
      },
      "source": [
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OM3GB9SV86n"
      },
      "source": [
        "# Plot training and test loss\n",
        "plt.plot(history.history['loss']) \n",
        "plt.plot(history.history['val_loss']) \n",
        "plt.title('Model loss') \n",
        "plt.ylabel('Loss') \n",
        "plt.xlabel('Epoch') \n",
        "plt.legend(['Train', 'Val'], loc='upper left') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1huEX1wx5bn"
      },
      "source": [
        "## 2. Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEFeuHMNvQxy"
      },
      "source": [
        "from keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZfVtoxky-8m"
      },
      "source": [
        "# Define the model\n",
        "input_layer = Input(shape=(X_train.shape[1],X_train.shape[2], X_train.shape[3]))\n",
        "conv_layer_1 = Conv2D(filters=8, kernel_size=(3, 3), activation='relu')(input_layer)\n",
        "bn_1 = BatchNormalization()(conv_layer_1) # We add this layer for BatchNormalization after the outup of a convolutional block. It allows to use greater learning rates\n",
        "maxpool_layer_1 = MaxPool2D(pool_size=(2, 2))(bn_1)\n",
        "conv_layer_2 = Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(maxpool_layer_1)\n",
        "bn_2 = BatchNormalization()(conv_layer_2)\n",
        "maxpool_layer_2 = MaxPool2D(pool_size=(2, 2))(bn_2)\n",
        "conv_layer_3 = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(maxpool_layer_2)\n",
        "bn_3 = BatchNormalization()(conv_layer_3)\n",
        "flatten_layer = Flatten()(bn_3)\n",
        "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
        "dropout = Dropout(rate=0.4)(dense_layer)\n",
        "output_layer = Dense(10, activation='softmax')(dropout)\n",
        "model= Model(inputs=input_layer, outputs=output_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG1aPlHFze4L"
      },
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIJo6a5kze4P"
      },
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train_fl, y_train_oh, epochs=20, batch_size=128,\n",
        "                    validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efPQki1pzZ9z"
      },
      "source": [
        "# Compile the model with new learning rate\n",
        "model= Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo6f1rlo0qWs"
      },
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train_fl, y_train_oh, epochs=20, batch_size=128,\n",
        "                    validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vsfjI7S2eCH"
      },
      "source": [
        "## 3. Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGqUrVUa063z"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator # Use web Albumentations.ai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjoPmQc32nmw"
      },
      "source": [
        "# Define data generator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip = True,\n",
        "    rotation_range = 10,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeXelmgq6IkN"
      },
      "source": [
        "# Function to convert a categorical class to its corresponding string\n",
        "def class_to_string(class_int):\n",
        "    classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\",\n",
        "               \"horse\", \"ship\", \"truck\"]\n",
        "    return classes[class_int]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgMWPtwx6fTF"
      },
      "source": [
        "plt.rcParams['figure.figsize'] = (10,10)  # Configure figure size for \n",
        "                                          # appropriate visualization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gp_wWxK3xc5"
      },
      "source": [
        "# Show generated data\n",
        "iterator = train_datagen.flow(X_train_fl, y_train_oh, batch_size=9)\n",
        "samples, labels = next(iterator)\n",
        "for i in range(9):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(samples[i], interpolation='none')\n",
        "    class_str = class_to_string(np.argmax(labels[i]))\n",
        "    plt.title(\"Class: \" + class_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj3tqJnv74Yn"
      },
      "source": [
        "# Compile the model with new learning rate\n",
        "model= Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW6z6oqi74Yu"
      },
      "source": [
        "# Train the model with 20 epochs\n",
        "batch_size = 20\n",
        "steps_per_epoch = X_train.shape[0] / batch_size\n",
        "history = model.fit_generator(train_datagen.flow(X_train_fl, y_train_oh, \n",
        "                                                 batch_size=batch_size), \n",
        "                              epochs=20,\n",
        "                              steps_per_epoch=steps_per_epoch, \n",
        "                              validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGTb98saDb6J"
      },
      "source": [
        "# Train the model with 100 epochs\n",
        "batch_size = 100\n",
        "steps_per_epoch = X_train.shape[0] / batch_size\n",
        "history = model.fit_generator(train_datagen.flow(X_train_fl, y_train_oh, \n",
        "                                                 batch_size=batch_size), \n",
        "                              epochs=100,\n",
        "                              steps_per_epoch=steps_per_epoch, \n",
        "                              validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBnVN5aoL_MJ"
      },
      "source": [
        "## 4. Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaqBTy80-PfP"
      },
      "source": [
        "# Import dependences for transfer learning\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from keras.models import Sequential\n",
        "from keras.layers import UpSampling2D\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoz0Dt6SV3QN"
      },
      "source": [
        "# Define the model\n",
        "resnet_model = ResNet50(weights='imagenet', include_top=False, # I discard the last layer of the ResNet50 model because I only have 10 classes instead of the 1000 classes of the original model\n",
        "                        input_shape=(256, 256, 3))\n",
        "model = Sequential()\n",
        "model.add(UpSampling2D((2,2))) # We increase the size of the image 3 times by a factor of 2 in x and y\n",
        "model.add(UpSampling2D((2,2))) # I increase the size of the images because, in order to use the transfer learning of the original model, my data should be as similar as possible\n",
        "model.add(UpSampling2D((2,2))) # to the data of ImageNet\n",
        "model.add(resnet_model)\n",
        "model.add(Flatten())\n",
        "model.add(BatchNormalization())\n",
        "# It is not mandatory to include new layers. In this case we include them as an example just to show that it's possible to add more layers\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arWFKw9yZF9k"
      },
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=RMSprop(lr=2e-5), loss='categorical_crossentropy', # The learning rate has to be small because I'm already using a pretrained model \n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4zR1iAiZQbO"
      },
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train_fl, y_train_oh, epochs=5, batch_size=50, # I'm not doing data augmentation in this case and that's why i use model.fit and not model.fit_generation\n",
        "                    validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp2NFx0MiXPz"
      },
      "source": [
        "# Train the model\n",
        "batch_size = 20\n",
        "steps_per_epoch = X_train.shape[0] / batch_size\n",
        "history = model.fit_generator(train_datagen.flow(X_train_fl, y_train_oh, \n",
        "                                                 batch_size=batch_size), # In this example we use transfer learning + data augmentation and that's why i have to use fit_generator\n",
        "                              epochs=5,\n",
        "                              steps_per_epoch=steps_per_epoch, \n",
        "                              validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9_3YAJta7x0"
      },
      "source": [
        "# Exercise 1: Experiment with other pre-trained models to evaluate results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2ZhdVS-iNOA"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CLwgtnlbKds"
      },
      "source": [
        "# Exercise 2: Use the concepts learnt to train a classifier for CIFAR100 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bWDRvYuPGE1"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}