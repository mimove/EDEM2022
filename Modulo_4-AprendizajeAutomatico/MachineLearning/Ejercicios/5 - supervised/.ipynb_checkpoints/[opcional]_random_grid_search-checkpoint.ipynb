{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score as metric\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook volveremos a trabajar con el dataset iris contenido en scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris;\n",
    "iris = load_iris();\n",
    "dat = iris.data;\n",
    "target = iris.target;\n",
    "target_names = iris.target_names;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Splitting\n",
    "\n",
    "Para poder realizar una correcta validación de un modelo de machine learning necesitamos dividir el conjunto de datos en varios subdatasets. Tenemos dos opciones:\n",
    "\n",
    "- **Conjunto de validación fijo**: Dividiremos el dataset en tres subconjuntos:\n",
    "\n",
    "    - Muestra de Entrenamiento (TRAINING) : Son los datos con los que se entrenan los modelos.\n",
    "    \n",
    "    - Muestra de Validación (VALIDATION) : Se emplea para seleccionar el mejor de los modelos entrenados cuando realizamos el ajuste de parámetros o metamodelización.\n",
    "    \n",
    "    - Muestra de Prueba (TEST) : Proporciona el error real esperado con el modelo seleccionado.\n",
    "\n",
    "\n",
    "- **Cross-validation**: Dividimos el dataset en dos subconjuntos: train y test. Para realizar la metamodelización haremos cross-validation sobre el conjunto de train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Conjunto de Validación Fijo\n",
    "\n",
    "Utilizaremos a modo de ejemplo los ratios habitualmente recomendados:\n",
    "\n",
    "- **Train**: 70%.\n",
    "\n",
    "\n",
    "- **Validación**: 15%.\n",
    "\n",
    "\n",
    "- **Test**: 15%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perc_values = [0.7, 0.15, 0.15];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usemos estos porcentajes y la función  train_test_split de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valtest, y_train, y_valtest = train_test_split(dat, target, test_size=perc_values[1] + perc_values[2], random_state=1);\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, test_size= perc_values[2] / (perc_values[1] + perc_values[2]), random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos las dimensiones de los tres conjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size = (105, 4)\n",
      "Train target size = (105,)\n",
      "Validation data size = (22, 4)\n",
      "Validation target size = (22,)\n",
      "Test data size = (23, 4)\n",
      "Test target size = (23,)\n"
     ]
    }
   ],
   "source": [
    "print('Train data size = ' + str(X_train.shape))\n",
    "print('Train target size = ' + str(y_train.shape))\n",
    "print('Validation data size = ' + str(X_val.shape))\n",
    "print('Validation target size = ' + str(y_val.shape))\n",
    "print('Test data size = ' + str(X_test.shape))\n",
    "print('Test target size = ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cross-Validation\n",
    "\n",
    "Utilizaremos a modo de ejemplo los siguientes ratios:\n",
    "\n",
    "- **Train**: 85%.\n",
    "\n",
    "\n",
    "- **Test**: 15%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perc_values = [0.85, 0.15];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usemos estos porcentajes y la función  train_test_split de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_cross, X_test_cross, y_train_cross, y_test_cross = train_test_split(dat, target, test_size = perc_values[1], random_state=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos las dimensiones de los dos conjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size = (127, 4)\n",
      "Ttrain target size = (127,)\n",
      "Test data size = (23, 4)\n",
      "Test target size = (23,)\n"
     ]
    }
   ],
   "source": [
    "print('Train data size = ' + str(X_train_cross.shape))\n",
    "print('Ttrain target size = ' + str(y_train_cross.shape))\n",
    "print('Test data size = ' + str(X_test_cross.shape))\n",
    "print('Test target size = ' + str(y_test_cross.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Validación Manual\n",
    "\n",
    "Vamos a realizar ahora la validación de un modelo de KNN para varios valores distintos del hiperparámetro k de forma manual, es decir, realizando explícitamente cada una de las llamadas. Para ello utilizaremos el método del **conjunto de validación fijo**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el modelo para k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_1 = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el modelo para k = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_7 = KNeighborsClassifier(n_neighbors=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos ambos modelos usando el conjunto de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k = 1\n",
    "knn_1.fit(X = X_train, y = y_train);\n",
    "\n",
    "# k = 7\n",
    "knn_7.fit(X = X_train, y = y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos las predicciones para train/val/test de cada uno de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k = 1\n",
    "pred_train_1 = knn_1.predict(X_train);\n",
    "pred_val_1 = knn_1.predict(X_val);\n",
    "\n",
    "# k = 7\n",
    "pred_train_7 = knn_7.predict(X_train);\n",
    "pred_val_7 = knn_7.predict(X_val);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora sus correspondientes accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k = 1\n",
    "acc_train_1 = metric(y_train, pred_train_1);\n",
    "acc_val_1 = metric(y_val, pred_val_1);\n",
    "\n",
    "# k = 7\n",
    "acc_train_7 = metric(y_train, pred_train_7);\n",
    "acc_val_7 = metric(y_val, pred_val_7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1 - accuracy train = 1.0\n",
      "k = 1 - accuracy val = 0.9545454545454546\n",
      "k = 7 - accuracy train = 0.9619047619047619\n",
      "k = 7 - accuracy val = 1.0\n"
     ]
    }
   ],
   "source": [
    "# k = 1\n",
    "print('k = 1 - accuracy train = ' + str(acc_train_1))\n",
    "print('k = 1 - accuracy val = ' + str(acc_val_1))\n",
    "\n",
    "# k = 7\n",
    "print('k = 7 - accuracy train = ' + str(acc_train_7))\n",
    "print('k = 7 - accuracy val = ' + str(acc_val_7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la vista de los resultados elegiríamos k = 7 por tener un menor error de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Curvas de Validación\n",
    "\n",
    "Vamos a construir ahora unos gráficos que nos permiten analizar el error de validación para toda una serie de valores del hiperparámetro k. Para ello utilizaremos el método de **cross-validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_values = np.arange(1, 21);\n",
    "train_scores, val_scores = validation_curve(KNeighborsClassifier(), X_train_cross, y_train_cross,\n",
    "                                       'n_neighbors', k_values,\n",
    "                                       scoring = make_scorer(metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostremos ahora los resultados obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmVkpFwszMyvlYmFmZqX+Cy4KF+1aybK4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c6308be7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_with_err(x, data, **kwargs):\n",
    "    mu, std = data.mean(1), data.std(1)\n",
    "    lines = plt.plot(x, mu, '-', **kwargs)\n",
    "    plt.fill_between(x, mu - std, mu + std, edgecolor='none',\n",
    "                     facecolor=lines[0].get_color(), alpha=0.2)\n",
    "\n",
    "plot_with_err(k_values, train_scores, label='training scores')\n",
    "plot_with_err(k_values, val_scores, label='validation scores')\n",
    "plt.xlabel('k'); plt.ylabel('accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A la vista de los resultado parece que el valor óptimo sería 8 o 10. Sin embargo, esta forma de elegir los hiperparámetros, basado en un enfoque visual, no parece demasiado científica ni escalable a un caso dónde se tengan varios hiperparámetros a optimizar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para solucionar estos problemas normalmente el método utilizado para elegir los hiperparámetros es otro, puramente analítico y no visual. Este método se conoce como **grid search** o **búsqueda en rejilla**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visto en clase, el método de grid search consta de los siguientes pasos:\n",
    "\n",
    "1. Elegimos una familia de modelos.\n",
    "\n",
    "2. Elegimos unos hiperparámetros a optimizar, les llamaremos par1 y par2. \n",
    "\n",
    "3. Para cada hiperparámetro, elegimos una serie de valores a probar.\n",
    "\n",
    "<img src=\"figures/grid.png\" width=\"50%\">\n",
    "\n",
    "4. Entrenamos nuestro modelo sobre el conjunto de train con los diferentes hiperparámetros haciendo todas las combinaciones posibles.\n",
    "\n",
    "5. Hacemos la predicción de los diferentes modelos sobre el conjunto de validación y calculamos el error con la métrica seleccionada.\n",
    "\n",
    "6. Escogemos el que mejor métrica obtenga y lo aplicamos sobre el conjunto de test para ver el error final esperado de nuestro modelo.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"figures/grid_3.png\" width=\"75%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cada uno de estos pasos aplicados a nuestro ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Elegimos una familia de modelos\n",
    "\n",
    "En nuestro estamos trabajando con los modelos KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Elegimos unos hiperparámetros a optimizar\n",
    "\n",
    "En nuestro caso optimizaremos el hiperparámetro k, o n_neighbors, el único relevante para los modelos de KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Para cada hiperparámetro, elegimos una serie de valores a probar\n",
    "\n",
    "En nuestro caso vamos a probar todos los valores del 1 al 20, es decir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_values = np.arange(1, 21);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Entrenamos nuestro modelo sobre el conjunto de train con los diferentes hiperparámetros haciendo todas las combinaciones posibles / 4.5 Hacemos la predicción de los diferentes modelos sobre el conjunto de validación y calculamos el error con la métrica seleccionada\n",
    "\n",
    "Realizaremos estos dos pasos en un único bloque de código.\n",
    "\n",
    "**Conjunto de validación fijo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_results = pd.DataFrame();\n",
    "for k in k_values:\n",
    "    # 4.4 Entrenar modelo\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X = X_train, y = y_train)\n",
    "    \n",
    "    # 4.5 Generar predicciones\n",
    "    pred_train = knn.predict(X_train);\n",
    "    pred_val = knn.predict(X_val);\n",
    "    \n",
    "    # 4.5 Calcular métricas de evaluación\n",
    "    acc_train = metric(y_train, pred_train);\n",
    "    acc_val = metric(y_val, pred_val);\n",
    "    \n",
    "    grid_results = grid_results.append(pd.DataFrame(data={'k':[k],'acc_train':[acc_train],'acc_val':[acc_val]}), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_val</th>\n",
       "      <th>k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.961905</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.971429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.971429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.971429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.961905</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.961905</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    acc_train   acc_val   k\n",
       "0    1.000000  0.954545   1\n",
       "1    0.971429  0.954545   2\n",
       "2    0.952381  0.954545   3\n",
       "3    0.961905  0.954545   4\n",
       "4    0.952381  0.954545   5\n",
       "5    0.971429  0.954545   6\n",
       "6    0.961905  1.000000   7\n",
       "7    0.971429  1.000000   8\n",
       "8    0.971429  1.000000   9\n",
       "9    0.971429  1.000000  10\n",
       "10   0.961905  1.000000  11\n",
       "11   0.961905  1.000000  12\n",
       "12   0.952381  1.000000  13\n",
       "13   0.952381  1.000000  14\n",
       "14   0.952381  1.000000  15\n",
       "15   0.952381  1.000000  16\n",
       "16   0.952381  1.000000  17\n",
       "17   0.952381  1.000000  18\n",
       "18   0.952381  1.000000  19\n",
       "19   0.952381  1.000000  20"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En vista de los resultados nos quedaríamos con cualquier valor de k entre 7 y 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-validation**\n",
    "\n",
    "Para este caso scikit-learn dispone de una función que hace los cálculos necesarios de forma automática, **GridSearchCV**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GridSearchCV in module sklearn.model_selection._search:\n",
      "\n",
      "class GridSearchCV(BaseSearchCV)\n",
      " |  Exhaustive search over specified parameter values for an estimator.\n",
      " |  \n",
      " |  Important members are fit, predict.\n",
      " |  \n",
      " |  GridSearchCV implements a \"fit\" and a \"score\" method.\n",
      " |  It also implements \"predict\", \"predict_proba\", \"decision_function\",\n",
      " |  \"transform\" and \"inverse_transform\" if they are implemented in the\n",
      " |  estimator used.\n",
      " |  \n",
      " |  The parameters of the estimator used to apply these methods are optimized\n",
      " |  by cross-validated grid-search over a parameter grid.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <grid_search>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : estimator object.\n",
      " |      This is assumed to implement the scikit-learn estimator interface.\n",
      " |      Either estimator needs to provide a ``score`` function,\n",
      " |      or ``scoring`` must be passed.\n",
      " |  \n",
      " |  param_grid : dict or list of dictionaries\n",
      " |      Dictionary with parameters names (string) as keys and lists of\n",
      " |      parameter settings to try as values, or a list of such\n",
      " |      dictionaries, in which case the grids spanned by each dictionary\n",
      " |      in the list are explored. This enables searching over any sequence\n",
      " |      of parameter settings.\n",
      " |  \n",
      " |  scoring : string, callable, list/tuple, dict or None, default: None\n",
      " |      A single string (see :ref:`scoring_parameter`) or a callable\n",
      " |      (see :ref:`scoring`) to evaluate the predictions on the test set.\n",
      " |  \n",
      " |      For evaluating multiple metrics, either give a list of (unique) strings\n",
      " |      or a dict with names as keys and callables as values.\n",
      " |  \n",
      " |      NOTE that when using custom scorers, each scorer should return a single\n",
      " |      value. Metric functions returning a list/array of values can be wrapped\n",
      " |      into multiple scorers that return one value each.\n",
      " |  \n",
      " |      See :ref:`multimetric_grid_search` for an example.\n",
      " |  \n",
      " |      If None, the estimator's score method is used.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      Number of jobs to run in parallel.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  pre_dispatch : int, or string, optional\n",
      " |      Controls the number of jobs that get dispatched during parallel\n",
      " |      execution. Reducing this number can be useful to avoid an\n",
      " |      explosion of memory consumption when more jobs get dispatched\n",
      " |      than CPUs can process. This parameter can be:\n",
      " |  \n",
      " |          - None, in which case all the jobs are immediately\n",
      " |            created and spawned. Use this for lightweight and\n",
      " |            fast-running jobs, to avoid delays due to on-demand\n",
      " |            spawning of the jobs\n",
      " |  \n",
      " |          - An int, giving the exact number of total jobs that are\n",
      " |            spawned\n",
      " |  \n",
      " |          - A string, giving an expression as a function of n_jobs,\n",
      " |            as in '2*n_jobs'\n",
      " |  \n",
      " |  iid : boolean, default=False\n",
      " |      If True, return the average score across folds, weighted by the number\n",
      " |      of samples in each test set. In this case, the data is assumed to be\n",
      " |      identically distributed across the folds, and the loss minimized is\n",
      " |      the total loss per sample, and not the mean loss across the folds.\n",
      " |  \n",
      " |      .. deprecated:: 0.22\n",
      " |          Parameter ``iid`` is deprecated in 0.22 and will be removed in 0.24\n",
      " |  \n",
      " |  cv : int, cross-validation generator or an iterable, optional\n",
      " |      Determines the cross-validation splitting strategy.\n",
      " |      Possible inputs for cv are:\n",
      " |  \n",
      " |      - None, to use the default 5-fold cross validation,\n",
      " |      - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      " |      - :term:`CV splitter`,\n",
      " |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |  \n",
      " |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      " |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      " |      other cases, :class:`KFold` is used.\n",
      " |  \n",
      " |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      " |      cross-validation strategies that can be used here.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      " |  \n",
      " |  refit : boolean, string, or callable, default=True\n",
      " |      Refit an estimator using the best found parameters on the whole\n",
      " |      dataset.\n",
      " |  \n",
      " |      For multiple metric evaluation, this needs to be a string denoting the\n",
      " |      scorer that would be used to find the best parameters for refitting\n",
      " |      the estimator at the end.\n",
      " |  \n",
      " |      Where there are considerations other than maximum score in\n",
      " |      choosing a best estimator, ``refit`` can be set to a function which\n",
      " |      returns the selected ``best_index_`` given ``cv_results_``. In that\n",
      " |      case, the ``best_estimator_`` and ``best_parameters_`` will be set\n",
      " |      according to the returned ``best_index_`` while the ``best_score_``\n",
      " |      attribute will not be available.\n",
      " |  \n",
      " |      The refitted estimator is made available at the ``best_estimator_``\n",
      " |      attribute and permits using ``predict`` directly on this\n",
      " |      ``GridSearchCV`` instance.\n",
      " |  \n",
      " |      Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      " |      ``best_score_`` and ``best_params_`` will only be available if\n",
      " |      ``refit`` is set and all of them will be determined w.r.t this specific\n",
      " |      scorer.\n",
      " |  \n",
      " |      See ``scoring`` parameter to know more about multiple metric\n",
      " |      evaluation.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |          Support for callable added.\n",
      " |  \n",
      " |  verbose : integer\n",
      " |      Controls the verbosity: the higher, the more messages.\n",
      " |  \n",
      " |  error_score : 'raise' or numeric\n",
      " |      Value to assign to the score if an error occurs in estimator fitting.\n",
      " |      If set to 'raise', the error is raised. If a numeric value is given,\n",
      " |      FitFailedWarning is raised. This parameter does not affect the refit\n",
      " |      step, which will always raise the error. Default is ``np.nan``.\n",
      " |  \n",
      " |  return_train_score : boolean, default=False\n",
      " |      If ``False``, the ``cv_results_`` attribute will not include training\n",
      " |      scores.\n",
      " |      Computing training scores is used to get insights on how different\n",
      " |      parameter settings impact the overfitting/underfitting trade-off.\n",
      " |      However computing the scores on the training set can be computationally\n",
      " |      expensive and is not strictly required to select the parameters that\n",
      " |      yield the best generalization performance.\n",
      " |  \n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn import svm, datasets\n",
      " |  >>> from sklearn.model_selection import GridSearchCV\n",
      " |  >>> iris = datasets.load_iris()\n",
      " |  >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      " |  >>> svc = svm.SVC()\n",
      " |  >>> clf = GridSearchCV(svc, parameters)\n",
      " |  >>> clf.fit(iris.data, iris.target)\n",
      " |  GridSearchCV(estimator=SVC(),\n",
      " |               param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n",
      " |  >>> sorted(clf.cv_results_.keys())\n",
      " |  ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
      " |   'param_C', 'param_kernel', 'params',...\n",
      " |   'rank_test_score', 'split0_test_score',...\n",
      " |   'split2_test_score', ...\n",
      " |   'std_fit_time', 'std_score_time', 'std_test_score']\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cv_results_ : dict of numpy (masked) ndarrays\n",
      " |      A dict with keys as column headers and values as columns, that can be\n",
      " |      imported into a pandas ``DataFrame``.\n",
      " |  \n",
      " |      For instance the below given table\n",
      " |  \n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n",
      " |      +============+===========+============+=================+===+=========+\n",
      " |      |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |  \n",
      " |      will be represented by a ``cv_results_`` dict of::\n",
      " |  \n",
      " |          {\n",
      " |          'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n",
      " |                                       mask = [False False False False]...)\n",
      " |          'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n",
      " |                                      mask = [ True  True False False]...),\n",
      " |          'param_degree': masked_array(data = [2.0 3.0 -- --],\n",
      " |                                       mask = [False False  True  True]...),\n",
      " |          'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
      " |          'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
      " |          'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
      " |          'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
      " |          'rank_test_score'    : [2, 4, 3, 1],\n",
      " |          'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
      " |          'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
      " |          'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
      " |          'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
      " |          'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n",
      " |          'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n",
      " |          'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n",
      " |          'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n",
      " |          'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
      " |          }\n",
      " |  \n",
      " |      NOTE\n",
      " |  \n",
      " |      The key ``'params'`` is used to store a list of parameter\n",
      " |      settings dicts for all the parameter candidates.\n",
      " |  \n",
      " |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      " |      ``std_score_time`` are all in seconds.\n",
      " |  \n",
      " |      For multi-metric evaluation, the scores for all the scorers are\n",
      " |      available in the ``cv_results_`` dict at the keys ending with that\n",
      " |      scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      " |      above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      " |  \n",
      " |  best_estimator_ : estimator\n",
      " |      Estimator that was chosen by the search, i.e. estimator\n",
      " |      which gave highest score (or smallest loss if specified)\n",
      " |      on the left out data. Not available if ``refit=False``.\n",
      " |  \n",
      " |      See ``refit`` parameter for more information on allowed values.\n",
      " |  \n",
      " |  best_score_ : float\n",
      " |      Mean cross-validated score of the best_estimator\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |      This attribute is not available if ``refit`` is a function.\n",
      " |  \n",
      " |  best_params_ : dict\n",
      " |      Parameter setting that gave the best results on the hold out data.\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |  best_index_ : int\n",
      " |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      " |      candidate parameter setting.\n",
      " |  \n",
      " |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      " |      the parameter setting for the best model, that gives the highest\n",
      " |      mean score (``search.best_score_``).\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |  scorer_ : function or a dict\n",
      " |      Scorer function used on the held out data to choose the best\n",
      " |      parameters for the model.\n",
      " |  \n",
      " |      For multi-metric evaluation, this attribute holds the validated\n",
      " |      ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      " |  \n",
      " |  n_splits_ : int\n",
      " |      The number of cross-validation splits (folds/iterations).\n",
      " |  \n",
      " |  refit_time_ : float\n",
      " |      Seconds used for refitting the best model on the whole dataset.\n",
      " |  \n",
      " |      This is present only if ``refit`` is not False.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The parameters selected are those that maximize the score of the left out\n",
      " |  data, unless an explicit score is passed in which case it is used instead.\n",
      " |  \n",
      " |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      " |  point in the grid (and not `n_jobs` times). This is done for efficiency\n",
      " |  reasons if individual jobs take very little time, but may raise errors if\n",
      " |  the dataset is large and not enough memory is available.  A workaround in\n",
      " |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      " |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      " |  n_jobs`.\n",
      " |  \n",
      " |  See Also\n",
      " |  ---------\n",
      " |  :class:`ParameterGrid`:\n",
      " |      generates all the combinations of a hyperparameter grid.\n",
      " |  \n",
      " |  :func:`sklearn.model_selection.train_test_split`:\n",
      " |      utility function to split the data into a development set usable\n",
      " |      for fitting a GridSearchCV instance and an evaluation set for\n",
      " |      its final evaluation.\n",
      " |  \n",
      " |  :func:`sklearn.metrics.make_scorer`:\n",
      " |      Make a scorer from a performance metric or loss function.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GridSearchCV\n",
      " |      BaseSearchCV\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator, param_grid, scoring=None, n_jobs=None, iid='deprecated', refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSearchCV:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Call decision_function on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``decision_function``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  fit(self, X, y=None, groups=None, **fit_params)\n",
      " |      Run fit with all sets of parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples, n_output) or (n_samples,), optional\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      groups : array-like, with shape (n_samples,), optional\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      " |          instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n",
      " |      \n",
      " |      **fit_params : dict of string -> object\n",
      " |          Parameters passed to the ``fit`` method of the estimator\n",
      " |  \n",
      " |  inverse_transform(self, Xt)\n",
      " |      Call inverse_transform on the estimator with the best found params.\n",
      " |      \n",
      " |      Only available if the underlying estimator implements\n",
      " |      ``inverse_transform`` and ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      Xt : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Call predict on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Call predict_log_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_log_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Call predict_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Returns the score on the given data, if the estimator has been refit.\n",
      " |      \n",
      " |      This uses the score defined by ``scoring`` where provided, and the\n",
      " |      ``best_estimator_.score`` method otherwise.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples, n_output) or (n_samples,), optional\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Call transform on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if the underlying estimator supports ``transform`` and\n",
      " |      ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseSearchCV:\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'n_neighbors': k_values}\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values validated for k hyperparameter: [1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "Validation accuracy: [0.94430769 0.92830769 0.96030769 0.95230769 0.95230769 0.95230769\n",
      " 0.96       0.976      0.976      0.96830769 0.95230769 0.96030769\n",
      " 0.968      0.968      0.96       0.96       0.96       0.96\n",
      " 0.96       0.96      ]\n",
      "Best score 0.976\n",
      "Best k: 8\n"
     ]
    }
   ],
   "source": [
    "grid_results_cv = GridSearchCV(KNeighborsClassifier(), param_grid, cv = 5)\n",
    "grid_results_cv.fit(X_train_cross, y_train_cross)\n",
    "print('Values validated for k hyperparameter: ' + str(grid_results_cv.cv_results_['param_n_neighbors']))\n",
    "print('Validation accuracy: ' + str(grid_results_cv.cv_results_['mean_test_score']))\n",
    "print('Best score ' + str(grid_results_cv.best_score_))\n",
    "print('Best k: ' + str(grid_results_cv.best_estimator_.n_neighbors))\n",
    "best_k = grid_results_cv.best_estimator_.n_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto, elegiríamos el valor k = 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Escogemos el que mejor métrica obtenga y lo aplicamos sobre el conjunto de test para ver el error final esperado de nuestro modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escogeremos el valor k = 8 como valor óptimo para nuestro modelo final. \n",
    "\n",
    "Entrenamos el modelo, para ello juntamos los conjuntos de train y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size = (105, 4)\n",
      "Train target size = (105,)\n",
      "Validation data size = (22, 4)\n",
      "Validation target size = (22,)\n",
      "Train data size = (127, 4)\n",
      "Train target size = (127,)\n"
     ]
    }
   ],
   "source": [
    "print('Train data size = ' + str(X_train.shape))\n",
    "print('Train target size = ' + str(y_train.shape))\n",
    "print('Validation data size = ' + str(X_val.shape))\n",
    "print('Validation target size = ' + str(y_val.shape))\n",
    "\n",
    "# Combinar train y validación\n",
    "X_train = np.concatenate((X_train,X_val), axis = 0)\n",
    "y_train = np.concatenate((y_train, y_val), axis = 0)\n",
    "\n",
    "del X_val, y_val\n",
    "\n",
    "print('Train data size = ' + str(X_train.shape))\n",
    "print('Train target size = ' + str(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=8, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar modelo\n",
    "knn = KNeighborsClassifier(n_neighbors = best_k)\n",
    "knn.fit(X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos predicciones:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generar predicciones\n",
    "pred_train = knn.predict(X_train);\n",
    "pred_test = knn.predict(X_test);    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos las métricas de evaluación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calcular métricas de evaluación\n",
    "acc_train = metric(y_train, pred_train);\n",
    "acc_test = metric(y_test, pred_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos los resultados finales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy train = 0.984251968503937\n",
      "accuracy test = 1.0\n"
     ]
    }
   ],
   "source": [
    "print('accuracy train = ' + str(acc_train))\n",
    "print('accuracy test = ' + str(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método de grid search anterior suele llamarse también Exhaustive Grid Search, porque prueba todos los posibles valores para la rejilla especificada. Cuándo se trabaja con un alto número de hiperparámetros, el uso de este método puede suponer probar un alto número de posibles combinaciones de valores.\n",
    "\n",
    "Por este motivo surgió una variante, llamada Random Grid Search, en la que no se prueban todas las posibles combinaciones indicadas en la rejilla, sino que se prueban puntos aleatorios que estén dentro de los rangos especificados. \n",
    "\n",
    "<img src=\"figures/grid_2.png\" width=\"50%\">\n",
    "\n",
    "El método Random Grid Search permite obtener un ahorro computacional y, en algunos casos como los de la imagen anterior, puede llevar también a conseguir mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar el método Random Grid Search usando scikit-learn es muy sencillo, ya que disponemos de la función RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomizedSearchCV in module sklearn.model_selection._search:\n",
      "\n",
      "class RandomizedSearchCV(BaseSearchCV)\n",
      " |  Randomized search on hyper parameters.\n",
      " |  \n",
      " |  RandomizedSearchCV implements a \"fit\" and a \"score\" method.\n",
      " |  It also implements \"predict\", \"predict_proba\", \"decision_function\",\n",
      " |  \"transform\" and \"inverse_transform\" if they are implemented in the\n",
      " |  estimator used.\n",
      " |  \n",
      " |  The parameters of the estimator used to apply these methods are optimized\n",
      " |  by cross-validated search over parameter settings.\n",
      " |  \n",
      " |  In contrast to GridSearchCV, not all parameter values are tried out, but\n",
      " |  rather a fixed number of parameter settings is sampled from the specified\n",
      " |  distributions. The number of parameter settings that are tried is\n",
      " |  given by n_iter.\n",
      " |  \n",
      " |  If all parameters are presented as a list,\n",
      " |  sampling without replacement is performed. If at least one parameter\n",
      " |  is given as a distribution, sampling with replacement is used.\n",
      " |  It is highly recommended to use continuous distributions for continuous\n",
      " |  parameters.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <randomized_parameter_search>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.14\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : estimator object.\n",
      " |      A object of that type is instantiated for each grid point.\n",
      " |      This is assumed to implement the scikit-learn estimator interface.\n",
      " |      Either estimator needs to provide a ``score`` function,\n",
      " |      or ``scoring`` must be passed.\n",
      " |  \n",
      " |  param_distributions : dict or list of dicts\n",
      " |      Dictionary with parameters names (string) as keys and distributions\n",
      " |      or lists of parameters to try. Distributions must provide a ``rvs``\n",
      " |      method for sampling (such as those from scipy.stats.distributions).\n",
      " |      If a list is given, it is sampled uniformly.\n",
      " |      If a list of dicts is given, first a dict is sampled uniformly, and\n",
      " |      then a parameter is sampled using that dict as above.\n",
      " |  \n",
      " |  n_iter : int, default=10\n",
      " |      Number of parameter settings that are sampled. n_iter trades\n",
      " |      off runtime vs quality of the solution.\n",
      " |  \n",
      " |  scoring : string, callable, list/tuple, dict or None, default: None\n",
      " |      A single string (see :ref:`scoring_parameter`) or a callable\n",
      " |      (see :ref:`scoring`) to evaluate the predictions on the test set.\n",
      " |  \n",
      " |      For evaluating multiple metrics, either give a list of (unique) strings\n",
      " |      or a dict with names as keys and callables as values.\n",
      " |  \n",
      " |      NOTE that when using custom scorers, each scorer should return a single\n",
      " |      value. Metric functions returning a list/array of values can be wrapped\n",
      " |      into multiple scorers that return one value each.\n",
      " |  \n",
      " |      See :ref:`multimetric_grid_search` for an example.\n",
      " |  \n",
      " |      If None, the estimator's score method is used.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      Number of jobs to run in parallel.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  pre_dispatch : int, or string, optional\n",
      " |      Controls the number of jobs that get dispatched during parallel\n",
      " |      execution. Reducing this number can be useful to avoid an\n",
      " |      explosion of memory consumption when more jobs get dispatched\n",
      " |      than CPUs can process. This parameter can be:\n",
      " |  \n",
      " |          - None, in which case all the jobs are immediately\n",
      " |            created and spawned. Use this for lightweight and\n",
      " |            fast-running jobs, to avoid delays due to on-demand\n",
      " |            spawning of the jobs\n",
      " |  \n",
      " |          - An int, giving the exact number of total jobs that are\n",
      " |            spawned\n",
      " |  \n",
      " |          - A string, giving an expression as a function of n_jobs,\n",
      " |            as in '2*n_jobs'\n",
      " |  \n",
      " |  iid : boolean, default=False\n",
      " |      If True, return the average score across folds, weighted by the number\n",
      " |      of samples in each test set. In this case, the data is assumed to be\n",
      " |      identically distributed across the folds, and the loss minimized is\n",
      " |      the total loss per sample, and not the mean loss across the folds.\n",
      " |  \n",
      " |      .. deprecated:: 0.22\n",
      " |          Parameter ``iid`` is deprecated in 0.22 and will be removed in 0.24\n",
      " |  \n",
      " |  cv : int, cross-validation generator or an iterable, optional\n",
      " |      Determines the cross-validation splitting strategy.\n",
      " |      Possible inputs for cv are:\n",
      " |  \n",
      " |      - None, to use the default 5-fold cross validation,\n",
      " |      - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      " |      - :term:`CV splitter`,\n",
      " |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |  \n",
      " |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      " |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      " |      other cases, :class:`KFold` is used.\n",
      " |  \n",
      " |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      " |      cross-validation strategies that can be used here.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      " |  \n",
      " |  refit : boolean, string, or callable, default=True\n",
      " |      Refit an estimator using the best found parameters on the whole\n",
      " |      dataset.\n",
      " |  \n",
      " |      For multiple metric evaluation, this needs to be a string denoting the\n",
      " |      scorer that would be used to find the best parameters for refitting\n",
      " |      the estimator at the end.\n",
      " |  \n",
      " |      Where there are considerations other than maximum score in\n",
      " |      choosing a best estimator, ``refit`` can be set to a function which\n",
      " |      returns the selected ``best_index_`` given the ``cv_results``. In that\n",
      " |      case, the ``best_estimator_`` and ``best_parameters_`` will be set\n",
      " |      according to the returned ``best_index_`` while the ``best_score_``\n",
      " |      attribute will not be available.\n",
      " |  \n",
      " |      The refitted estimator is made available at the ``best_estimator_``\n",
      " |      attribute and permits using ``predict`` directly on this\n",
      " |      ``RandomizedSearchCV`` instance.\n",
      " |  \n",
      " |      Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      " |      ``best_score_`` and ``best_params_`` will only be available if\n",
      " |      ``refit`` is set and all of them will be determined w.r.t this specific\n",
      " |      scorer.\n",
      " |  \n",
      " |      See ``scoring`` parameter to know more about multiple metric\n",
      " |      evaluation.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |          Support for callable added.\n",
      " |  \n",
      " |  verbose : integer\n",
      " |      Controls the verbosity: the higher, the more messages.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default=None\n",
      " |      Pseudo random number generator state used for random uniform sampling\n",
      " |      from lists of possible values instead of scipy.stats distributions.\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  error_score : 'raise' or numeric\n",
      " |      Value to assign to the score if an error occurs in estimator fitting.\n",
      " |      If set to 'raise', the error is raised. If a numeric value is given,\n",
      " |      FitFailedWarning is raised. This parameter does not affect the refit\n",
      " |      step, which will always raise the error. Default is ``np.nan``.\n",
      " |  \n",
      " |  return_train_score : boolean, default=False\n",
      " |      If ``False``, the ``cv_results_`` attribute will not include training\n",
      " |      scores.\n",
      " |      Computing training scores is used to get insights on how different\n",
      " |      parameter settings impact the overfitting/underfitting trade-off.\n",
      " |      However computing the scores on the training set can be computationally\n",
      " |      expensive and is not strictly required to select the parameters that\n",
      " |      yield the best generalization performance.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cv_results_ : dict of numpy (masked) ndarrays\n",
      " |      A dict with keys as column headers and values as columns, that can be\n",
      " |      imported into a pandas ``DataFrame``.\n",
      " |  \n",
      " |      For instance the below given table\n",
      " |  \n",
      " |      +--------------+-------------+-------------------+---+---------------+\n",
      " |      | param_kernel | param_gamma | split0_test_score |...|rank_test_score|\n",
      " |      +==============+=============+===================+===+===============+\n",
      " |      |    'rbf'     |     0.1     |       0.80        |...|       2       |\n",
      " |      +--------------+-------------+-------------------+---+---------------+\n",
      " |      |    'rbf'     |     0.2     |       0.90        |...|       1       |\n",
      " |      +--------------+-------------+-------------------+---+---------------+\n",
      " |      |    'rbf'     |     0.3     |       0.70        |...|       1       |\n",
      " |      +--------------+-------------+-------------------+---+---------------+\n",
      " |  \n",
      " |      will be represented by a ``cv_results_`` dict of::\n",
      " |  \n",
      " |          {\n",
      " |          'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],\n",
      " |                                        mask = False),\n",
      " |          'param_gamma'  : masked_array(data = [0.1 0.2 0.3], mask = False),\n",
      " |          'split0_test_score'  : [0.80, 0.90, 0.70],\n",
      " |          'split1_test_score'  : [0.82, 0.50, 0.70],\n",
      " |          'mean_test_score'    : [0.81, 0.70, 0.70],\n",
      " |          'std_test_score'     : [0.01, 0.20, 0.00],\n",
      " |          'rank_test_score'    : [3, 1, 1],\n",
      " |          'split0_train_score' : [0.80, 0.92, 0.70],\n",
      " |          'split1_train_score' : [0.82, 0.55, 0.70],\n",
      " |          'mean_train_score'   : [0.81, 0.74, 0.70],\n",
      " |          'std_train_score'    : [0.01, 0.19, 0.00],\n",
      " |          'mean_fit_time'      : [0.73, 0.63, 0.43],\n",
      " |          'std_fit_time'       : [0.01, 0.02, 0.01],\n",
      " |          'mean_score_time'    : [0.01, 0.06, 0.04],\n",
      " |          'std_score_time'     : [0.00, 0.00, 0.00],\n",
      " |          'params'             : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],\n",
      " |          }\n",
      " |  \n",
      " |      NOTE\n",
      " |  \n",
      " |      The key ``'params'`` is used to store a list of parameter\n",
      " |      settings dicts for all the parameter candidates.\n",
      " |  \n",
      " |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      " |      ``std_score_time`` are all in seconds.\n",
      " |  \n",
      " |      For multi-metric evaluation, the scores for all the scorers are\n",
      " |      available in the ``cv_results_`` dict at the keys ending with that\n",
      " |      scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      " |      above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      " |  \n",
      " |  best_estimator_ : estimator\n",
      " |      Estimator that was chosen by the search, i.e. estimator\n",
      " |      which gave highest score (or smallest loss if specified)\n",
      " |      on the left out data. Not available if ``refit=False``.\n",
      " |  \n",
      " |      For multi-metric evaluation, this attribute is present only if\n",
      " |      ``refit`` is specified.\n",
      " |  \n",
      " |      See ``refit`` parameter for more information on allowed values.\n",
      " |  \n",
      " |  best_score_ : float\n",
      " |      Mean cross-validated score of the best_estimator.\n",
      " |  \n",
      " |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      " |      ``False``. See ``refit`` parameter for more information.\n",
      " |  \n",
      " |      This attribute is not available if ``refit`` is a function.\n",
      " |  \n",
      " |  best_params_ : dict\n",
      " |      Parameter setting that gave the best results on the hold out data.\n",
      " |  \n",
      " |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      " |      ``False``. See ``refit`` parameter for more information.\n",
      " |  \n",
      " |  best_index_ : int\n",
      " |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      " |      candidate parameter setting.\n",
      " |  \n",
      " |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      " |      the parameter setting for the best model, that gives the highest\n",
      " |      mean score (``search.best_score_``).\n",
      " |  \n",
      " |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      " |      ``False``. See ``refit`` parameter for more information.\n",
      " |  \n",
      " |  scorer_ : function or a dict\n",
      " |      Scorer function used on the held out data to choose the best\n",
      " |      parameters for the model.\n",
      " |  \n",
      " |      For multi-metric evaluation, this attribute holds the validated\n",
      " |      ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      " |  \n",
      " |  n_splits_ : int\n",
      " |      The number of cross-validation splits (folds/iterations).\n",
      " |  \n",
      " |  refit_time_ : float\n",
      " |      Seconds used for refitting the best model on the whole dataset.\n",
      " |  \n",
      " |      This is present only if ``refit`` is not False.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The parameters selected are those that maximize the score of the held-out\n",
      " |  data, according to the scoring parameter.\n",
      " |  \n",
      " |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      " |  parameter setting(and not `n_jobs` times). This is done for efficiency\n",
      " |  reasons if individual jobs take very little time, but may raise errors if\n",
      " |  the dataset is large and not enough memory is available.  A workaround in\n",
      " |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      " |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      " |  n_jobs`.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  :class:`GridSearchCV`:\n",
      " |      Does exhaustive search over a grid of parameters.\n",
      " |  \n",
      " |  :class:`ParameterSampler`:\n",
      " |      A generator over parameter settings, constructed from\n",
      " |      param_distributions.\n",
      " |  \n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> from sklearn.model_selection import RandomizedSearchCV\n",
      " |  >>> from scipy.stats import uniform\n",
      " |  >>> iris = load_iris()\n",
      " |  >>> logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n",
      " |  ...                               random_state=0)\n",
      " |  >>> distributions = dict(C=uniform(loc=0, scale=4),\n",
      " |  ...                      penalty=['l2', 'l1'])\n",
      " |  >>> clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
      " |  >>> search = clf.fit(iris.data, iris.target)\n",
      " |  >>> search.best_params_\n",
      " |  {'C': 2..., 'penalty': 'l1'}\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomizedSearchCV\n",
      " |      BaseSearchCV\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator, param_distributions, n_iter=10, scoring=None, n_jobs=None, iid='deprecated', refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None, error_score=nan, return_train_score=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSearchCV:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Call decision_function on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``decision_function``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  fit(self, X, y=None, groups=None, **fit_params)\n",
      " |      Run fit with all sets of parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples, n_output) or (n_samples,), optional\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      groups : array-like, with shape (n_samples,), optional\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      " |          instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n",
      " |      \n",
      " |      **fit_params : dict of string -> object\n",
      " |          Parameters passed to the ``fit`` method of the estimator\n",
      " |  \n",
      " |  inverse_transform(self, Xt)\n",
      " |      Call inverse_transform on the estimator with the best found params.\n",
      " |      \n",
      " |      Only available if the underlying estimator implements\n",
      " |      ``inverse_transform`` and ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      Xt : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Call predict on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Call predict_log_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_log_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Call predict_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Returns the score on the given data, if the estimator has been refit.\n",
      " |      \n",
      " |      This uses the score defined by ``scoring`` where provided, and the\n",
      " |      ``best_estimator_.score`` method otherwise.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples, n_output) or (n_samples,), optional\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Call transform on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if the underlying estimator supports ``transform`` and\n",
      " |      ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseSearchCV:\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomizedSearchCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos los rangos de valores sobre los que queremos escoger puntos de forma aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': k_values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y llamamos a RandomizedSearchCV donde el parametro n_iter indica cuantos valores del hiperparámetro k, elegidos de forma aleatoria, queremos probar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values validated for k hyperparameter: [20 19]\n",
      "Validation accuracy: [0.96 0.96]\n",
      "Best score 0.96\n",
      "Best k: 20\n"
     ]
    }
   ],
   "source": [
    "grid_results_cv = RandomizedSearchCV(KNeighborsClassifier(), param_grid, n_iter = 2, cv = 5)\n",
    "grid_results_cv.fit(X_train_cross, y_train_cross)\n",
    "print('Values validated for k hyperparameter: ' + str(grid_results_cv.cv_results_['param_n_neighbors']))\n",
    "print('Validation accuracy: ' + str(grid_results_cv.cv_results_['mean_test_score']))\n",
    "print('Best score ' + str(grid_results_cv.best_score_))\n",
    "print('Best k: ' + str(grid_results_cv.best_estimator_.n_neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso el valor elegido será k = 7."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
