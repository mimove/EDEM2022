{"cells":[{"cell_type":"markdown","metadata":{"id":"BNkZfzfxGZ0z"},"source":["# Window Partitioning Exercises"]},{"cell_type":"markdown","source":["## Prerrequisites"],"metadata":{"id":"AQieQ5pkGfNm"}},{"cell_type":"markdown","source":["Install Spark and Java in VM"],"metadata":{"id":"HelxRmCPGpql"}},{"cell_type":"code","source":["# install Java8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","# download spark3.0.1\n","!wget -q https://apache.osuosl.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop2.tgz"],"metadata":{"id":"9Cn3c-ywGtDV","executionInfo":{"status":"ok","timestamp":1670445554186,"user_tz":-60,"elapsed":71963,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["ls -l # check the .tgz is there"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D95sNcJfGvyV","executionInfo":{"status":"ok","timestamp":1670445554187,"user_tz":-60,"elapsed":15,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"}},"outputId":"092edcbe-4d24-48e3-9d2c-3a3e6f128aec"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["total 267684\n","drwxr-xr-x 1 root root      4096 Dec  6 14:35 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n","-rw-r--r-- 1 root root 274099817 Oct 15 10:53 spark-3.3.1-bin-hadoop2.tgz\n"]}]},{"cell_type":"code","source":["# unzip it\n","!tar xf spark-3.3.1-bin-hadoop2.tgz"],"metadata":{"id":"qtBMGi7EGvwN","executionInfo":{"status":"ok","timestamp":1670445557676,"user_tz":-60,"elapsed":3494,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!pip install -q findspark"],"metadata":{"id":"6JO331NrGvtt","executionInfo":{"status":"ok","timestamp":1670445561954,"user_tz":-60,"elapsed":4280,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Defining the environment"],"metadata":{"id":"02epIDkbG24d"}},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop2\"\n","os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[*] pyspark-shell\""],"metadata":{"id":"qmON5zHJG4-m","executionInfo":{"status":"ok","timestamp":1670445561954,"user_tz":-60,"elapsed":3,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WgvNJQOAGZ00"},"source":["Start Spark Session\n","\n","---"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"siaPZq4XGZ00","executionInfo":{"status":"ok","timestamp":1670445571832,"user_tz":-60,"elapsed":9881,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"}},"outputId":"1189ddcc-c17e-4825-a376-658b82c8cf5d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'3.3.1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["import findspark\n","findspark.init(\"spark-3.3.1-bin-hadoop2\")# SPARK_HOME\n","\n","from pyspark.sql import SparkSession\n","\n","# create the session\n","spark = SparkSession \\\n","        .builder \\\n","        .appName(\"Window Partitioning Exercises\") \\\n","        .master(\"local[*]\") \\\n","        .getOrCreate()\n","\n","spark.version"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"nsBkpLh6GZ01","executionInfo":{"status":"ok","timestamp":1670445574706,"user_tz":-60,"elapsed":2878,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"}},"outputId":"9dddc53c-0608-4482-d9ef-90321e1df827"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7f0a5d2149d0>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://35542b16111e:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Window Partitioning Exercises</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":7}],"source":["spark"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Bqu4fQnNGZ02","executionInfo":{"status":"ok","timestamp":1670445574707,"user_tz":-60,"elapsed":6,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"}}},"outputs":[],"source":["# For Pandas conversion optimization\n","spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"-9DDmYQKGZ02","executionInfo":{"status":"ok","timestamp":1670445574707,"user_tz":-60,"elapsed":5,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"}}},"outputs":[],"source":["# Import sql functions\n","from pyspark.sql.functions import *"]},{"cell_type":"markdown","source":["Download datasets"],"metadata":{"id":"NYrtXWZIHKMt"}},{"cell_type":"code","source":["!mkdir -p dataset\n","!wget -q https://raw.githubusercontent.com/paponsro/spark_edem_2022/master/datasets/characters.csv -P /dataset"],"metadata":{"id":"2lkKBm3CHL-l","executionInfo":{"status":"ok","timestamp":1670445577386,"user_tz":-60,"elapsed":2683,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sxWVtHu5GZ02"},"source":["## Window Partitioning Exercises"]},{"cell_type":"markdown","source":["1. Load characters.csv to a DataFrame. Then, get the (two) tallest characters per species and per homeworld planet. Select the name, height, and species/homeworld in their case.\n","2. Get the height difference for each character with respect to the smallest one in the same homeworld."],"metadata":{"id":"rZt5nAVLRmeF"}},{"cell_type":"code","source":[],"metadata":{"id":"DF-WHVfjRq49","executionInfo":{"status":"ok","timestamp":1670445577386,"user_tz":-60,"elapsed":6,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"}}},"execution_count":10,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"ff1af5cda0bea4fe5c4ebc1f94ab9f13d8998f98d08e16d8aba48673b9d00116"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}